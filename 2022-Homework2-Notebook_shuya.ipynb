{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2\n",
    "\n",
    "Please note that this is a group project (2 students per group). If you do not have a group yet, please email georgiana.ifrim@ucd.ie to be assigned to a group.\n",
    "\n",
    "Please upload to Brightspace a **.zip** archive containing your Jupyter Notebook with solutions and all data required to reproduce your solutions. \n",
    "\n",
    "Please also prepare a **requirements.txt** file which lists all the packages that you have used for your homework, one package per line. This will allow us to install all required packages.\n",
    "\n",
    "Please name your .zip archive using your full names and student ids as follows - **Firstname1_Lastname1_id1_Firstname2_Lastname2_id2_COMP47350_Homework1.zip**. Please only 1 person in the team submit this zip file to Brightspace (no need for each member to submit a duplicate of the zip).\n",
    "\n",
    "For your Notebook, please split the code and explanations into cells so it is easy to see and read the results of each step of your solution. Please remember to name your variables and methods with self-explanatory names. Please remember to write comments and where needed, justifications, for the decisions you make and code you write. \n",
    "\n",
    "Your code and analysis is like a story that awaits to be read, please make it a nice and clear story. Always start with an introduction about the problem and your understanding of the problem domain and data analytics solution and describe your steps and your findings from each step.\n",
    "\n",
    "The accepted file formats for the homework are:\n",
    "    - .ipynb\n",
    "    - .zip\n",
    "    - .pdf\n",
    "    - .csv\n",
    "    \n",
    "Please aim to keep the whole code for Homework2 in a single notebook. In case you need to first prepare the data and prefer to do this in a separate notebook, before starting on Homework2, you can do this, but to the Homework2 notebook add a short summary of what was done to prepare the data. Please document the structure of your files in a README.txt file, so your submission folder and files are easy to navigate for an outsider.\n",
    "\n",
    "Usage of external tools/files is discouraged for portability reasons. Files in any other format but mentioned above can be used but will not considered for the submission (including .doc, .rar, .7z, .pages, .xlsx, .tex etc.). \n",
    "Any image format is allowed to be used as far as the images appear embedded in your report (.ipynb or .pdf or .html).\n",
    "\n",
    "**Deadline: Monday, 8 May, 2023, midnight.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**This homework focuses on training and evaluating prediction models for a particular problem and dataset.**\n",
    "The data comes from the Centers for Disease Control and Prevention (CDC: https://covid.cdc.gov/covid-data-tracker/). CDC is a USA health protection agency and is in charge of collecting data about the COVID-19 pandemic, and in particular, tracking cases, deaths, and trends of COVID-19 in the United States. CDC collects and makes public deidentified individual-case data on a daily basis, submitted using standardized case reporting forms. In this analysis, we focus on using the data collected by CDC to build a data analytics solution for death risk prediction. \n",
    "\n",
    "The dataset we work with is a sample of the public data released by CDC, where the outcome for the target feature **death_yn** is known (i.e., either 'yes' or 'no'):\n",
    "https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf\n",
    "\n",
    "The goal in this homework is to work with the data to build and evaluate prediction models that capture the relationship between the descriptive features and the target feature **death_yn**. For this homework you are asked to use the same dataset allocated to you in Homework1 (you can use your cleaned/prepared CSV from Homework1 or start from the raw dataset, clean it according to concepts covered in the lectures/labs, then use it for training prediction models). To use the 2 individual files allocated for Homework1, you can merge them first, then clean the resulting dataset, before starting on Homework2 requirements.\n",
    " \n",
    "There are 5 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 25% shown as [25]. The total that can be achieved is 100.\n",
    "\n",
    "\n",
    "(1). [25] **Data Understanding and Preparation:** Exploring relationships between feature pairs and selecting/transforming promising features based on a given training set.\n",
    "\n",
    "    - (1.1) Split the dataset into two datasets: 70% training and 30% test. Keep the test set aside. \n",
    "    - (1.2) On the training set:\n",
    "        - Plot the correlations between all the continuous features (if any). Discuss what you observe in these plots.\n",
    "        - For each continuous feature, plot its interaction with the target feature (a plot for each pair of   continuous feature and target feature). Discuss what you observe from these plots, e.g., which continuous features seem to be better at predicting the target feature? Choose a subset of continuous features you find promising (if any). Justify your choices.\n",
    "        - For each categorical feature, plot its pairwise interaction with the target feature. Discuss what  knowledge you gain from these plots, e.g., which categorical features seem to be better at predicting the target feature? Choose a subset of categorical features you find promising (if any). Justify your choices.\n",
    "      \n",
    "    \n",
    "(2). [15] **Predictive Modeling:** Linear Regression.  \n",
    "\n",
    "    - (2.1) On the training set, train a linear regression model to predict the target feature, using only the  descriptive features selected in exercise (1) above. \n",
    "    - (2.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature).    \n",
    "    - (2.3) Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (2.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated random train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "(3). [15] **Predictive Modeling:** Logistic Regression.  \n",
    "\n",
    "    - (3.1) On the training set, train a logistic regression model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (3.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model).    \n",
    "    - (3.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (3.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "    \n",
    "(4). [20] **Predictive Modeling:** Random Forest.  \n",
    "\n",
    "    - (4.1) On the training set, train a random forest model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (4.2) Can you interpret the random forest model? Discuss any knowledge you can gain in regard of the working of this model.   \n",
    "    - (4.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (4.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and to the Random Forest out-of-sample error and discuss your findings.\n",
    "    \n",
    "(5). [25] **Improving Predictive Models.**\n",
    "\n",
    "    - (5.1) Which model of the ones trained above performs better at predicting the target feature? Is it more   accurate than a simple model that always predicts the majority class (i.e., if 'no' is the majority class in your dataset, the simple model always predicts 'no' for the target feature)? Justify your answers.\n",
    "    - (5.2) Summarise your understanding of the problem and of your predictive modeling results so far. Can you think of any new ideas to improve the best model so far (e.g., by using furher data prep such as: feature selection, feature re-scaling, creating new features, combining predictive models, or using other domain knowledge)? Please show how your ideas actually work in practice (with code), by training and evaluating your proposed models. Summarise your findings so far. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "## Cian Belton: 19321726\n",
    "## Shuya Ikeo: \n",
    "### Use data from homework 1 to build and evaluate prediction models that capture the relationship between features and target features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports for this notebook\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Writing installed packages to requirements.txt file\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0: Merging and Cleaning Data\n",
    "Will merge both our datasets into a new csv file named 'merged_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now the dataset we will work on for the rest of this notebook\n",
    "As we already had homework 1 to clean the data, we will use Cian's data quality plan to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_month', 'res_state', 'state_fips_code', 'res_county',\n",
       "       'county_fips_code', 'age_group', 'sex', 'race', 'ethnicity',\n",
       "       'case_positive_specimen_interval', 'case_onset_interval', 'process',\n",
       "       'exposure_yn', 'current_status', 'symptom_status', 'hosp_yn', 'icu_yn',\n",
       "       'death_yn', 'underlying_conditions_yn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_month', 'res_state', 'res_county', 'age_group', 'sex', 'race',\n",
       "       'ethnicity', 'current_status', 'hosp_yn', 'icu_yn', 'death_yn',\n",
       "       'underlying_conditions_yn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_column(df, column_to_drop):\n",
    "    if column_to_drop in df.columns:\n",
    "        df = df.drop(column_to_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "df = drop_column(df, \"case_positive_specimen_interval\")\n",
    "df = drop_column(df, \"case_onset_interval\")\n",
    "df = drop_column(df, \"county_fips_code\")\n",
    "df = drop_column(df, \"state_fips_code\")\n",
    "df = drop_column(df, \"symptom_status\")\n",
    "df = drop_column(df, \"exposure_yn\")\n",
    "df = drop_column(df, \"process\")\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change unknown/null values to Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column(df, column_to_clean):\n",
    "    if column_to_clean in df.columns:\n",
    "        df[column_to_clean] = df[column_to_clean].fillna('Missing')\n",
    "        df[column_to_clean] = df[column_to_clean].replace('Unknown', 'Missing')\n",
    "        df[column_to_clean] = df[column_to_clean].replace('missing', 'Missing')\n",
    "    return df\n",
    "\n",
    "df = clean_column(df, \"res_county\")\n",
    "df = clean_column(df, \"res_state\")\n",
    "df = clean_column(df, \"age_group\")\n",
    "df = clean_column(df, \"sex\")\n",
    "df = clean_column(df, \"race\")\n",
    "df = clean_column(df, \"ethnicity\")\n",
    "df = clean_column(df, \"hosp_yn\")\n",
    "df = clean_column(df, \"icu_yn\")\n",
    "df = clean_column(df, \"underlying_conditions_yn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_month                  0\n",
       "res_state                   0\n",
       "res_county                  0\n",
       "age_group                   0\n",
       "sex                         0\n",
       "race                        0\n",
       "ethnicity                   0\n",
       "current_status              0\n",
       "hosp_yn                     0\n",
       "icu_yn                      0\n",
       "death_yn                    0\n",
       "underlying_conditions_yn    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()+df.eq('Unknown').sum()+df.eq('missing').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 12)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Rows:\n",
    "Drop row that has a missing value for res_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39999, 12)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df['res_state'].eq('Missing')].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows that fail integrity test 3 from Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39993, 12)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop the that failed test 3- The number of rows that failed the test to check that every\n",
    "# paitent in the ICU was also recorded as being in hospital are: 6\n",
    "df.drop(df[(df['icu_yn'] == 'Yes') & (df['hosp_yn'] == 'No')].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Data Types:\n",
    "All columns remaining are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39993 entries, 0 to 39999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   case_month                39993 non-null  object\n",
      " 1   res_state                 39993 non-null  object\n",
      " 2   res_county                39993 non-null  object\n",
      " 3   age_group                 39993 non-null  object\n",
      " 4   sex                       39993 non-null  object\n",
      " 5   race                      39993 non-null  object\n",
      " 6   ethnicity                 39993 non-null  object\n",
      " 7   current_status            39993 non-null  object\n",
      " 8   hosp_yn                   39993 non-null  object\n",
      " 9   icu_yn                    39993 non-null  object\n",
      " 10  death_yn                  39993 non-null  object\n",
      " 11  underlying_conditions_yn  39993 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39993 entries, 0 to 39999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   case_month                39993 non-null  category\n",
      " 1   res_state                 39993 non-null  category\n",
      " 2   res_county                39993 non-null  category\n",
      " 3   age_group                 39993 non-null  category\n",
      " 4   sex                       39993 non-null  category\n",
      " 5   race                      39993 non-null  category\n",
      " 6   ethnicity                 39993 non-null  category\n",
      " 7   current_status            39993 non-null  category\n",
      " 8   hosp_yn                   39993 non-null  category\n",
      " 9   icu_yn                    39993 non-null  category\n",
      " 10  death_yn                  39993 non-null  category\n",
      " 11  underlying_conditions_yn  39993 non-null  category\n",
      "dtypes: category(12)\n",
      "memory usage: 864.0 KB\n"
     ]
    }
   ],
   "source": [
    "##convert all remaining categorical columns to category type\n",
    "df = df.astype('category')\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in 3 target features from Homework 1 that have a high correlation with death risk\n",
    "#### 1. high_risk_category: hosp_yn=Yes and age_group=65+ years\n",
    "\n",
    "- high_risk_category= hosp_yn= Yes and age_group is 65+ years\n",
    "- Was this person hospitilized and were they older than 65?\n",
    "    - Yes if true\n",
    "    - To do this create binary(yes/no) encoding for in 65+ age group or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Yes\n",
       "1         No\n",
       "2         No\n",
       "3        Yes\n",
       "4         No\n",
       "        ... \n",
       "39995     No\n",
       "39996     No\n",
       "39997     No\n",
       "39998     No\n",
       "39999     No\n",
       "Name: high_risk_category, Length: 39993, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['high_risk_category'] = np.where((df[\"age_group\"] == \"65+ years\") & (df[\"hosp_yn\"] == \"Yes\"), \"Yes\", \"No\")\n",
    "df['high_risk_category']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. before_feb_2021\n",
    "- before_feb_2021= case_month<2021_02\n",
    "- If somone had covid before February of 2021 they were more likely to die from it\n",
    "    - Get case fatality ratio before and after this month for all age groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do this convert to continuous feature of type int\n",
    "# remove the -\n",
    "df[\"case_month\"]=df[\"case_month\"].str.replace('-', '')\n",
    "#convert to int\n",
    "df[\"case_month\"]=df[\"case_month\"].astype(int)\n",
    "df['before_feb_2021'] = np.where((df[\"case_month\"]<202102), \"Yes\", \"No\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. high_risk_state:\n",
    "- If the person is from NY, CA, IL or FL then this will return yes \n",
    "- This feature is useful as these 4 states contain 40.7% of the total deaths but only 23.7% of the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         No\n",
       "1        Yes\n",
       "2        Yes\n",
       "3         No\n",
       "4        Yes\n",
       "        ... \n",
       "39995    Yes\n",
       "39996     No\n",
       "39997    Yes\n",
       "39998     No\n",
       "39999     No\n",
       "Name: high_risk_state, Length: 39993, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['high_risk_state'] = np.where((df[\"res_state\"] == \"CA\") | (df[\"res_state\"] == \"NY\") | (df[\"res_state\"] == \"IL\") | (df[\"res_state\"] == \"FL\"), \"Yes\", \"No\")\n",
    "df['high_risk_state']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in an additional feature to group the states in order to graph the data more easily:\n",
    "### 4. Region:\n",
    "- Dividing the states into distinct regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_region(state):\n",
    "    east = ['ME', 'NH', 'VT', 'MA', 'RI', 'CT', 'NY', 'NJ', 'PA', 'DE', 'MD', 'DC']\n",
    "    south = ['WV', 'VA', 'NC', 'SC', 'GA', 'FL', 'AL', 'TN', 'MS', 'AR', 'LA', 'TX', 'OK']\n",
    "    midwest = ['OH', 'IN', 'IL', 'MI', 'WI', 'MN', 'IA', 'MO', 'ND', 'SD', 'NE', 'KS']\n",
    "    west = ['WA', 'OR', 'CA', 'NV', 'ID', 'MT', 'WY', 'CO', 'NM', 'AZ', 'UT']\n",
    "    other = ['AK', 'HI']\n",
    "    \n",
    "    if state in east:\n",
    "        return 'East'\n",
    "    elif state in south:\n",
    "        return 'South'\n",
    "    elif state in midwest:\n",
    "        return 'Midwest'\n",
    "    elif state in west:\n",
    "        return 'West'\n",
    "    elif state in other:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return 'Missing'\n",
    "    \n",
    "# Apply the mapping function to create a new column 'region'\n",
    "df['region'] = df['res_state'].apply(state_to_region)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Categorical types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39993 entries, 0 to 39999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   case_month                39993 non-null  int64   \n",
      " 1   res_state                 39993 non-null  category\n",
      " 2   res_county                39993 non-null  category\n",
      " 3   age_group                 39993 non-null  category\n",
      " 4   sex                       39993 non-null  category\n",
      " 5   race                      39993 non-null  category\n",
      " 6   ethnicity                 39993 non-null  category\n",
      " 7   current_status            39993 non-null  category\n",
      " 8   hosp_yn                   39993 non-null  category\n",
      " 9   icu_yn                    39993 non-null  category\n",
      " 10  death_yn                  39993 non-null  category\n",
      " 11  underlying_conditions_yn  39993 non-null  category\n",
      " 12  high_risk_category        39993 non-null  category\n",
      " 13  before_feb_2021           39993 non-null  category\n",
      " 14  high_risk_state           39993 non-null  category\n",
      " 15  region                    39993 non-null  category\n",
      "dtypes: category(15), int64(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "columns=df[['high_risk_category','before_feb_2021','high_risk_state','region']].columns\n",
    "for column in columns:\n",
    "    df[column] = df[column].astype('category') \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>res_county</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>current_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>underlying_conditions_yn</th>\n",
       "      <th>high_risk_category</th>\n",
       "      <th>before_feb_2021</th>\n",
       "      <th>high_risk_state</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202111</td>\n",
       "      <td>NV</td>\n",
       "      <td>CLARK</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202201</td>\n",
       "      <td>FL</td>\n",
       "      <td>HERNANDO</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202012</td>\n",
       "      <td>CA</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202012</td>\n",
       "      <td>TN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202003</td>\n",
       "      <td>NY</td>\n",
       "      <td>KINGS</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Male</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>202112</td>\n",
       "      <td>IN</td>\n",
       "      <td>ALLEN</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Probable Case</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>202012</td>\n",
       "      <td>PA</td>\n",
       "      <td>SCHUYLKILL</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202012</td>\n",
       "      <td>IL</td>\n",
       "      <td>COOK</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202012</td>\n",
       "      <td>CA</td>\n",
       "      <td>SANTA CLARA</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202201</td>\n",
       "      <td>MA</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_month res_state   res_county       age_group     sex     race  \\\n",
       "0      202111        NV        CLARK       65+ years    Male    White   \n",
       "1      202201        FL     HERNANDO       65+ years    Male    White   \n",
       "2      202012        CA  LOS ANGELES  50 to 64 years    Male    White   \n",
       "3      202012        TN      Missing       65+ years    Male    White   \n",
       "4      202003        NY        KINGS  50 to 64 years    Male  Missing   \n",
       "5      202112        IN        ALLEN       65+ years    Male    White   \n",
       "6      202012        PA   SCHUYLKILL       65+ years    Male    White   \n",
       "7      202012        IL         COOK       65+ years  Female    White   \n",
       "8      202012        CA  SANTA CLARA       65+ years  Female  Missing   \n",
       "9      202201        MA      SUFFOLK       65+ years    Male    White   \n",
       "\n",
       "             ethnicity             current_status  hosp_yn   icu_yn death_yn  \\\n",
       "0  Non-Hispanic/Latino  Laboratory-confirmed case      Yes  Missing      Yes   \n",
       "1  Non-Hispanic/Latino  Laboratory-confirmed case       No  Missing      Yes   \n",
       "2      Hispanic/Latino  Laboratory-confirmed case       No  Missing      Yes   \n",
       "3  Non-Hispanic/Latino  Laboratory-confirmed case      Yes      Yes      Yes   \n",
       "4              Missing  Laboratory-confirmed case      Yes  Missing      Yes   \n",
       "5  Non-Hispanic/Latino              Probable Case       No  Missing      Yes   \n",
       "6  Non-Hispanic/Latino  Laboratory-confirmed case  Missing  Missing      Yes   \n",
       "7      Hispanic/Latino  Laboratory-confirmed case       No  Missing      Yes   \n",
       "8              Missing  Laboratory-confirmed case       No  Missing      Yes   \n",
       "9  Non-Hispanic/Latino  Laboratory-confirmed case      Yes  Missing      Yes   \n",
       "\n",
       "  underlying_conditions_yn high_risk_category before_feb_2021 high_risk_state  \\\n",
       "0                  Missing                Yes              No              No   \n",
       "1                  Missing                 No              No             Yes   \n",
       "2                  Missing                 No             Yes             Yes   \n",
       "3                      Yes                Yes             Yes              No   \n",
       "4                      Yes                 No             Yes             Yes   \n",
       "5                  Missing                 No              No              No   \n",
       "6                  Missing                 No             Yes              No   \n",
       "7                  Missing                 No             Yes             Yes   \n",
       "8                  Missing                 No             Yes             Yes   \n",
       "9                  Missing                Yes              No              No   \n",
       "\n",
       "    region  \n",
       "0     West  \n",
       "1    South  \n",
       "2     West  \n",
       "3    South  \n",
       "4     East  \n",
       "5  Midwest  \n",
       "6     East  \n",
       "7  Midwest  \n",
       "8     West  \n",
       "9     East  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "df.to_csv('cleaned_merged_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The exported CSV file is contains our merged data, is cleaned according to the main points of Homework 1 and contains 4 new target features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39993 entries, 0 to 39992\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   case_month                39993 non-null  int64 \n",
      " 1   res_state                 39993 non-null  object\n",
      " 2   res_county                39993 non-null  object\n",
      " 3   age_group                 39993 non-null  object\n",
      " 4   sex                       39993 non-null  object\n",
      " 5   race                      39993 non-null  object\n",
      " 6   ethnicity                 39993 non-null  object\n",
      " 7   current_status            39993 non-null  object\n",
      " 8   hosp_yn                   39993 non-null  object\n",
      " 9   icu_yn                    39993 non-null  object\n",
      " 10  death_yn                  39993 non-null  object\n",
      " 11  underlying_conditions_yn  39993 non-null  object\n",
      " 12  high_risk_category        39993 non-null  object\n",
      " 13  before_feb_2021           39993 non-null  object\n",
      " 14  high_risk_state           39993 non-null  object\n",
      " 15  region                    39993 non-null  object\n",
      "dtypes: int64(1), object(15)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_merged_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data set into for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \n",
    "    train, test = train_test_split(dataset, test_size = 0.3, random_state=42)\n",
    "    #random_state can be anything\n",
    "    \n",
    "    train_x = train.drop([\"death_yn\"], axis = 1)\n",
    "    train_y = train[\"death_yn\"]\n",
    "    test_x = test.drop([\"death_yn\"], axis = 1)\n",
    "    test_y = test[\"death_yn\"]\n",
    "    return train, train_x, train_y, test, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the correlations between all the continuous features (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_x, train_y, test, test_x, test_y = split_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = train_x.select_dtypes([\"Int64\"])\n",
    "categorical_columns = train_x.select_dtypes([\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_matrix = continuous_columns.corr()\n",
    "\n",
    "# 1 is the maximum of the correlation\n",
    "\n",
    "#sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.jointplot(x = \"case_positive_specimen_interval\", y = \"case_onset_interval\", data = train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss what you observe from this.\n",
    "\n",
    "When it comes to machine learning, The input values should no have any relartion ship because it will distract the quality of the prediction model. In this hear map, they have -0.021 co relation which means they have very weak corelation. So I do not get rid of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each continuous feature, plot its interaction with the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(x, target):\n",
    "    for column in x.columns:\n",
    "       \n",
    "        plt.scatter(x[column], target)\n",
    "        plt.title(f\"Scatter plot of {column} vs {target.name}\")\n",
    "        plt.ylabel(target.name)\n",
    "        plt.xlabel(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_interaction(continuous_columns, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for_bar = train[['case_onset_interval', 'death_yn']]\n",
    "#grouped = for_bar.groupby(['case_onset_interval', 'death_yn'])['death_yn'].count().unstack()\n",
    "#ax = grouped.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "#ax.set_xlabel('case onset interval')\n",
    "#ax.set_ylabel('Number of Cases')\n",
    "#ax.set_title('Number of Deaths by case onset Interval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for_bar = train[['case_positive_specimen_interval', 'death_yn']]\n",
    "#grouped = for_bar.groupby(['case_positive_specimen_interval', 'death_yn'])['death_yn'].count().unstack()\n",
    "#ax = grouped.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "#ax.set_xlabel('case_positive_specimen_interval')\n",
    "#ax.set_ylabel('Number of Cases')\n",
    "#ax.set_title('Number of Deaths by case_positive_specimen_interval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss my findings.\n",
    "\n",
    "When it come to which is better for prediction models, i would say the relation with \"case_onset_interval\". That is because they have some interaction which the case_on_interval goes big, the death number is up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each categorical feature, plot its pairwise interaction with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_interaction(categorical_columns, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotrelation(column):\n",
    "    for_bar = train[[column, 'death_yn']]\n",
    "    grouped = for_bar.groupby([column, 'death_yn'])['death_yn'].count().unstack()\n",
    "    ax = grouped.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Number of Cases')\n",
    "    ax.set_title(f'Number of Deaths by {column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_columns.columns:\n",
    "    plotrelation(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Too see the relation visualized\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def perform_chi_square_test(df, cat_columns, target_col):\n",
    "    for cat_col in cat_columns:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[cat_col], df[target_col])\n",
    "        \n",
    "        # Perform the chi-square test of independence\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Print the results\n",
    "        print(f\"Chi-square test results for {cat_col} and {target_col}:\")\n",
    "        print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "        print(f\"P-value: {p:.4f}\")\n",
    "        print(f\"Degrees of freedom: {dof}\")\n",
    "        print(\"Expected frequencies:\")\n",
    "        print(expected)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test results for case_month and death_yn:\n",
      "Chi-square statistic: 4073.8382\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 34\n",
      "Expected frequencies:\n",
      "[[7.50884086e+00 2.49115914e+00]\n",
      " [7.50884086e-01 2.49115914e-01]\n",
      " [3.38648723e+02 1.12351277e+02]\n",
      " [8.04196857e+02 2.66803143e+02]\n",
      " [3.57420825e+02 1.18579175e+02]\n",
      " [3.29638114e+02 1.09361886e+02]\n",
      " [5.75928094e+02 1.91071906e+02]\n",
      " [3.96466798e+02 1.31533202e+02]\n",
      " [3.41652259e+02 1.13347741e+02]\n",
      " [6.34497053e+02 2.10502947e+02]\n",
      " [1.34483340e+03 4.46166601e+02]\n",
      " [1.78485147e+03 5.92148527e+02]\n",
      " [1.45971866e+03 4.84281336e+02]\n",
      " [5.24867976e+02 1.74132024e+02]\n",
      " [4.87323772e+02 1.61676228e+02]\n",
      " [4.34761886e+02 1.44238114e+02]\n",
      " [2.07994892e+02 6.90051081e+01]\n",
      " [1.18639686e+02 3.93603143e+01]\n",
      " [3.94214145e+02 1.30785855e+02]\n",
      " [9.86661690e+02 3.27338310e+02]\n",
      " [8.24470727e+02 2.73529273e+02]\n",
      " [4.97085265e+02 1.64914735e+02]\n",
      " [6.73543026e+02 2.23456974e+02]\n",
      " [1.54531945e+03 5.12680550e+02]\n",
      " [2.72495835e+03 9.04041650e+02]\n",
      " [4.06979175e+02 1.35020825e+02]\n",
      " [1.24646758e+02 4.13532417e+01]\n",
      " [2.56051473e+02 8.49485265e+01]\n",
      " [6.50265619e+02 2.15734381e+02]\n",
      " [4.03975639e+02 1.34024361e+02]\n",
      " [4.58039293e+02 1.51960707e+02]\n",
      " [4.24249509e+02 1.40750491e+02]\n",
      " [2.38030255e+02 7.89697446e+01]\n",
      " [1.89222790e+02 6.27772102e+01]\n",
      " [7.35866405e+01 2.44133595e+01]]\n",
      "\n",
      "\n",
      "Chi-square test results for res_state and death_yn:\n",
      "Chi-square statistic: 8914.5893\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 48\n",
      "Expected frequencies:\n",
      "[[4.50530452e+00 1.49469548e+00]\n",
      " [1.16387033e+02 3.86129666e+01]\n",
      " [1.65945383e+02 5.50546169e+01]\n",
      " [5.49647151e+02 1.82352849e+02]\n",
      " [7.90680943e+02 2.62319057e+02]\n",
      " [1.63692731e+02 5.43072692e+01]\n",
      " [7.35866405e+01 2.44133595e+01]\n",
      " [7.35866405e+01 2.44133595e+01]\n",
      " [1.66696267e+03 5.53037328e+02]\n",
      " [1.95229862e+01 6.47701375e+00]\n",
      " [7.50884086e-01 2.49115914e-01]\n",
      " [5.63163065e+01 1.86836935e+01]\n",
      " [1.92226326e+02 6.37736739e+01]\n",
      " [2.80830648e+02 9.31693517e+01]\n",
      " [1.17588448e+03 3.90115521e+02]\n",
      " [5.36131238e+02 1.77868762e+02]\n",
      " [7.18596071e+02 2.38403929e+02]\n",
      " [3.22880157e+01 1.07119843e+01]\n",
      " [2.62058546e+02 8.69414538e+01]\n",
      " [7.05080157e+02 2.33919843e+02]\n",
      " [1.71952456e+02 5.70475442e+01]\n",
      " [5.52650688e+02 1.83349312e+02]\n",
      " [1.01369352e+03 3.36306483e+02]\n",
      " [9.83658153e+01 3.26341847e+01]\n",
      " [2.47791749e+01 8.22082515e+00]\n",
      " [8.71025540e+01 2.88974460e+01]\n",
      " [1.89898585e+03 6.30014145e+02]\n",
      " [1.87721022e+01 6.22789784e+00]\n",
      " [6.45760314e+01 2.14239686e+01]\n",
      " [1.85618546e+03 6.15814538e+02]\n",
      " [3.97968566e+01 1.32031434e+01]\n",
      " [2.95848330e+02 9.81516699e+01]\n",
      " [2.24213988e+03 7.43860118e+02]\n",
      " [1.10980668e+03 3.68193320e+02]\n",
      " [2.37279371e+02 7.87206287e+01]\n",
      " [2.50044401e+02 8.29555992e+01]\n",
      " [4.84320236e+02 1.60679764e+02]\n",
      " [4.35512770e+01 1.44487230e+01]\n",
      " [1.20892338e+02 4.01076621e+01]\n",
      " [3.28887230e+02 1.09112770e+02]\n",
      " [2.46289980e+02 8.17100196e+01]\n",
      " [1.11356110e+03 3.69438900e+02]\n",
      " [2.49293517e+02 8.27064833e+01]\n",
      " [4.48277800e+02 1.48722200e+02]\n",
      " [3.75442043e+00 1.24557957e+00]\n",
      " [3.15371316e+01 1.04628684e+01]\n",
      " [6.23233792e+01 2.06766208e+01]\n",
      " [3.16122200e+02 1.04877800e+02]\n",
      " [2.55300589e+01 8.46994106e+00]]\n",
      "\n",
      "\n",
      "Chi-square test results for res_county and death_yn:\n",
      "Chi-square statistic: 10414.6316\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 907\n",
      "Expected frequencies:\n",
      "[[  4.50530452   1.49469548]\n",
      " [100.61846758  33.38153242]\n",
      " [  0.75088409   0.24911591]\n",
      " ...\n",
      " [ 23.27740668   7.72259332]\n",
      " [ 51.81100196  17.18899804]\n",
      " [ 25.53005894   8.46994106]]\n",
      "\n",
      "\n",
      "Chi-square test results for age_group and death_yn:\n",
      "Chi-square statistic: 14686.6592\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 4\n",
      "Expected frequencies:\n",
      "[[2788.03261297  924.96738703]\n",
      " [8179.38035363 2713.61964637]\n",
      " [3343.68683694 1109.31316306]\n",
      " [6520.67740668 2163.32259332]\n",
      " [ 189.22278978   62.77721022]]\n",
      "\n",
      "\n",
      "Chi-square test results for sex and death_yn:\n",
      "Chi-square statistic: 314.0432\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 2\n",
      "Expected frequencies:\n",
      "[[10585.96385069  3512.03614931]\n",
      " [ 9915.42436149  3289.57563851]\n",
      " [  519.61178782   172.38821218]]\n",
      "\n",
      "\n",
      "Chi-square test results for race and death_yn:\n",
      "Chi-square statistic: 1113.0361\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 6\n",
      "Expected frequencies:\n",
      "[[7.88428291e+01 2.61571709e+01]\n",
      " [4.46025147e+02 1.47974853e+02]\n",
      " [2.27292613e+03 7.54073870e+02]\n",
      " [5.05420079e+03 1.67679921e+03]\n",
      " [1.71201572e+02 5.67984283e+01]\n",
      " [2.25265226e+00 7.47347741e-01]\n",
      " [1.29955509e+04 4.31144912e+03]]\n",
      "\n",
      "\n",
      "Chi-square test results for ethnicity and death_yn:\n",
      "Chi-square statistic: 1372.0353\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 2\n",
      "Expected frequencies:\n",
      "[[ 1842.66954813   611.33045187]\n",
      " [ 6574.7410609   2181.2589391 ]\n",
      " [12603.58939096  4181.41060904]]\n",
      "\n",
      "\n",
      "Chi-square test results for current_status and death_yn:\n",
      "Chi-square statistic: 235.7301\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 1\n",
      "Expected frequencies:\n",
      "[[17899.57485265  5938.42514735]\n",
      " [ 3121.42514735  1035.57485265]]\n",
      "\n",
      "\n",
      "Chi-square test results for hosp_yn and death_yn:\n",
      "Chi-square statistic: 11696.6158\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 2\n",
      "Expected frequencies:\n",
      "[[ 7025.27151277  2330.72848723]\n",
      " [10401.24636542  3450.75363458]\n",
      " [ 3594.48212181  1192.51787819]]\n",
      "\n",
      "\n",
      "Chi-square test results for icu_yn and death_yn:\n",
      "Chi-square statistic: 1911.5382\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 2\n",
      "Expected frequencies:\n",
      "[[19172.32337917  6360.67662083]\n",
      " [ 1308.04007859   433.95992141]\n",
      " [  540.63654224   179.36345776]]\n",
      "\n",
      "\n",
      "Chi-square test results for underlying_conditions_yn and death_yn:\n",
      "Chi-square statistic: 631.8485\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 2\n",
      "Expected frequencies:\n",
      "[[1.91715725e+04 6.36042750e+03]\n",
      " [2.10247544e+01 6.97524558e+00]\n",
      " [1.82840275e+03 6.06597250e+02]]\n",
      "\n",
      "\n",
      "Chi-square test results for high_risk_category and death_yn:\n",
      "Chi-square statistic: 10755.1456\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 1\n",
      "Expected frequencies:\n",
      "[[18158.62986248  6024.37013752]\n",
      " [ 2862.37013752   949.62986248]]\n",
      "\n",
      "\n",
      "Chi-square test results for before_feb_2021 and death_yn:\n",
      "Chi-square statistic: 2202.2988\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 1\n",
      "Expected frequencies:\n",
      "[[12644.88801572  4195.11198428]\n",
      " [ 8376.11198428  2778.88801572]]\n",
      "\n",
      "\n",
      "Chi-square test results for high_risk_state and death_yn:\n",
      "Chi-square statistic: 1451.0889\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 1\n",
      "Expected frequencies:\n",
      "[[16040.38585462  5321.61414538]\n",
      " [ 4980.61414538  1652.38585462]]\n",
      "\n",
      "\n",
      "Chi-square test results for region and death_yn:\n",
      "Chi-square statistic: 1503.0208\n",
      "P-value: 0.0000\n",
      "Degrees of freedom: 5\n",
      "Expected frequencies:\n",
      "[[6.08591552e+03 2.01908448e+03]\n",
      " [5.15857367e+03 1.71142633e+03]\n",
      " [7.66652652e+02 2.54347348e+02]\n",
      " [4.50530452e+00 1.49469548e+00]\n",
      " [6.29916660e+03 2.08983340e+03]\n",
      " [2.70618625e+03 8.97813752e+02]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_chi_square_test(train, train_x, \"death_yn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings from the visualized relationship between the categorical columns and the target columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will wrte this later. But i wont drop any columns based on this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the categorical features into numerical values\n",
    "\n",
    "We basically have to do this work as our data set has only categorical columns and values so we can not do the mathematical job without this encoding. We will use two different method to do so. The first one is one hot encoding which creates the number of the values columns from the column, if the value matches the column, the value will be 1, if not, 0. This method is really usuful when the column does not have too many distinct values and specific orders. The other method is label encoding which put the numbers for distinct values in the column. This method is usuful when there is specific orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non-order-columns, we will use one_hot encoding\n",
    "\n",
    "def one_hot_encode(df, column):\n",
    " \n",
    "    encoded_df = pd.get_dummies(df[column], prefix=column)\n",
    "    return pd.concat([df, encoded_df], axis=1).drop(column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_with_order(df, column, order):\n",
    "    \n",
    "    # Create a dictionary mapping each value to its encoded value\n",
    "    encoding_dict = {value: index for index, value in enumerate(order)}\n",
    "\n",
    "    # Apply the encoding to the column\n",
    "    encoded_col = df[column].map(encoding_dict)\n",
    "\n",
    "    # Replace the original column with the encoded column\n",
    "    df[column] = encoded_col\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for one-hot encoding\n",
    "columns_for_onehot = [\"sex\", \"race\", \"ethnicity\", \"current_status\", \"hosp_yn\", \"icu_yn\", \"underlying_conditions_yn\", \"high_risk_category\", \"before_feb_2021\", \"high_risk_state\", \"region\"]\n",
    "\n",
    "# Define columns for label encoding\n",
    "columns_for_label = [\"age_group\"]\n",
    "\n",
    "# Other columns(If i wanna add other columns)\n",
    "#other_columns = \n",
    "\n",
    "df_encoded = df.copy()\n",
    "for column in columns_for_onehot:\n",
    "    df_encoded = one_hot_encode(df_encoded, column)\n",
    "\n",
    "# Apply label encoding\n",
    "order = [\"0 - 17 years\", \"18 to 49 years\", \"50 to 64 years\", \"65+ years\", \"Missing\"]\n",
    "df_encoded = label_encode_with_order(df_encoded, columns_for_label[0], order)\n",
    "\n",
    "# Concatenate the encoded columns and drop some columns.\n",
    "df_for_model = df_encoded.drop([\"case_month\", \"res_state\", \"res_county\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_group</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sex_Missing</th>\n",
       "      <th>race_American Indian/Alaska Native</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Missing</th>\n",
       "      <th>race_Multiple/Other</th>\n",
       "      <th>...</th>\n",
       "      <th>before_feb_2021_No</th>\n",
       "      <th>before_feb_2021_Yes</th>\n",
       "      <th>high_risk_state_No</th>\n",
       "      <th>high_risk_state_Yes</th>\n",
       "      <th>region_East</th>\n",
       "      <th>region_Midwest</th>\n",
       "      <th>region_Missing</th>\n",
       "      <th>region_Other</th>\n",
       "      <th>region_South</th>\n",
       "      <th>region_West</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39993 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age_group death_yn  sex_Female  sex_Male  sex_Missing  \\\n",
       "0              3      Yes           0         1            0   \n",
       "1              3      Yes           0         1            0   \n",
       "2              2      Yes           0         1            0   \n",
       "3              3      Yes           0         1            0   \n",
       "4              2      Yes           0         1            0   \n",
       "...          ...      ...         ...       ...          ...   \n",
       "39988          3       No           1         0            0   \n",
       "39989          0       No           0         1            0   \n",
       "39990          1       No           1         0            0   \n",
       "39991          1       No           0         1            0   \n",
       "39992          2       No           1         0            0   \n",
       "\n",
       "       race_American Indian/Alaska Native  race_Asian  race_Black  \\\n",
       "0                                       0           0           0   \n",
       "1                                       0           0           0   \n",
       "2                                       0           0           0   \n",
       "3                                       0           0           0   \n",
       "4                                       0           0           0   \n",
       "...                                   ...         ...         ...   \n",
       "39988                                   0           0           0   \n",
       "39989                                   0           0           0   \n",
       "39990                                   0           0           0   \n",
       "39991                                   0           0           0   \n",
       "39992                                   0           0           0   \n",
       "\n",
       "       race_Missing  race_Multiple/Other  ...  before_feb_2021_No  \\\n",
       "0                 0                    0  ...                   1   \n",
       "1                 0                    0  ...                   1   \n",
       "2                 0                    0  ...                   0   \n",
       "3                 0                    0  ...                   0   \n",
       "4                 1                    0  ...                   0   \n",
       "...             ...                  ...  ...                 ...   \n",
       "39988             0                    0  ...                   0   \n",
       "39989             0                    0  ...                   1   \n",
       "39990             0                    0  ...                   1   \n",
       "39991             1                    0  ...                   0   \n",
       "39992             0                    0  ...                   0   \n",
       "\n",
       "       before_feb_2021_Yes  high_risk_state_No  high_risk_state_Yes  \\\n",
       "0                        0                   1                    0   \n",
       "1                        0                   0                    1   \n",
       "2                        1                   0                    1   \n",
       "3                        1                   1                    0   \n",
       "4                        1                   0                    1   \n",
       "...                    ...                 ...                  ...   \n",
       "39988                    1                   0                    1   \n",
       "39989                    0                   1                    0   \n",
       "39990                    0                   0                    1   \n",
       "39991                    1                   1                    0   \n",
       "39992                    1                   1                    0   \n",
       "\n",
       "       region_East  region_Midwest  region_Missing  region_Other  \\\n",
       "0                0               0               0             0   \n",
       "1                0               0               0             0   \n",
       "2                0               0               0             0   \n",
       "3                0               0               0             0   \n",
       "4                1               0               0             0   \n",
       "...            ...             ...             ...           ...   \n",
       "39988            0               0               0             0   \n",
       "39989            0               1               0             0   \n",
       "39990            1               0               0             0   \n",
       "39991            0               0               0             0   \n",
       "39992            0               0               0             0   \n",
       "\n",
       "       region_South  region_West  \n",
       "0                 0            1  \n",
       "1                 1            0  \n",
       "2                 0            1  \n",
       "3                 1            0  \n",
       "4                 0            0  \n",
       "...             ...          ...  \n",
       "39988             1            0  \n",
       "39989             0            0  \n",
       "39990             0            0  \n",
       "39991             1            0  \n",
       "39992             1            0  \n",
       "\n",
       "[39993 rows x 38 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion from encoding\n",
    "\n",
    "As it is shown above, we encoded our columns into numerical values for preparation to create the machine learning models. We did one-hot encoding for most of the columns and did label encoding for the column \"age_group\". This is because the age_group labe has a specific order from yonget to older and the others do not.\n",
    "\n",
    "We also got rid of some columns from the data set for modeling.\n",
    "\n",
    "- case_month\n",
    "We got rid of the case_month column from the final data set for modeling because there are too many unique values which doesn't help to make the model.\n",
    "\n",
    "- res_state\n",
    "We got rid of the res_state column from the final data set for modeling because there are too many unique values which doesn't help to make the model. And there is a column named \"high_risk_state\" which duplicates the column but also specify the high risk states.\n",
    "\n",
    "- res_county\n",
    "We got rid of the res_county column from the final data set for modeling because there are too many unique values which doesn't help to make the model. And there is a column named \"region\" which is a duplicate of res_state but it is also for res_county column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
